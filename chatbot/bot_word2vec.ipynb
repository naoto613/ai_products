{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d1e961e034ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m#     boya_words = pickle.load(f)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "from janome.tokenizer import Tokenizer\n",
    "import collections\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "with open(\"bot.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
    "    bot_original = f.read()\n",
    "\n",
    "bot = re.sub(\"[\\n]\", \"。\", bot_original) # | と全角半角スペース、「」と改行の削除\n",
    "bot = re.sub(\"[｜ 　]\", \"\", bot) # | と全角半角スペース、「」と改行の削除\n",
    "\n",
    "# 終わり記号の重複を除去\n",
    "bot = bot.replace('！。', '！')\n",
    "bot = bot.replace('？。', '？')\n",
    "bot = bot.replace('♪。', '♪')\n",
    "bot = bot.replace('？*', '？')\n",
    "bot = bot.replace('♪？', '♪')\n",
    "bot = bot.replace('。*', '。')\n",
    "\n",
    "seperator = \"。\"  # 。をセパレータに指定\n",
    "bot_list = re.split('[。！？♪]', bot)  # セパレーターを使って文章をリストに分割する\n",
    "bot_list.pop() # 最後の要素は空の文字列になるので、削除\n",
    "bot_list.pop(0)\n",
    "bot_list = [x+seperator for x in bot_list]  # 文章の最後に。を追加\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "bot_words = []\n",
    "for sentence in bot_list:\n",
    "    bot_words.append(t.tokenize(sentence, wakati=True))   # 文章ごとに単語に分割し、リストに格納\n",
    "    \n",
    "with open('bot_words.pickle', mode='wb') as f:  # pickleに保存\n",
    "    pickle.dump(bot_words, f)\n",
    "\n",
    "# 読み込み用\n",
    "# with open('bot_words.pickle', mode='rb') as f:\n",
    "#     bot_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size : 中間層のニューロン数\n",
    "# min_count : この値以下の出現回数の単語を無視\n",
    "# window : 対象単語を中心とした前後の単語数\n",
    "# iter : epochs数\n",
    "# sg : skip-gramを使うかどうか 0:CBOW 1:skip-gram\n",
    "model = word2vec.Word2Vec(bot_words,\n",
    "                          size=100,\n",
    "                          min_count=5,\n",
    "                          window=5,\n",
    "                          iter=20,\n",
    "                          sg = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2266, 100)\n",
      "[[-3.6608776e-01  6.9012302e-01  4.0331030e-01 ...  3.3114445e-01\n",
      "   8.5808325e-01 -2.0029101e-01]\n",
      " [-2.5216025e-01  1.3797539e-01  7.4951988e-01 ... -7.6588849e-03\n",
      "  -3.3250514e-01 -1.5308677e-03]\n",
      " [-5.4016936e-01  2.2724479e-01  1.0155753e+00 ... -7.2973377e-01\n",
      "   6.4893931e-01 -2.9359201e-01]\n",
      " ...\n",
      " [ 9.5886040e-05 -9.9446207e-02  7.1058877e-02 ...  8.4064431e-02\n",
      "  -2.0338913e-02 -3.4190239e-03]\n",
      " [-6.0925116e-03 -1.2996139e-01  9.9249281e-02 ...  1.1599533e-01\n",
      "  -5.2975252e-02  7.3325372e-04]\n",
      " [ 6.7871367e-04  3.4442514e-02 -4.7413964e-02 ...  1.6589938e-01\n",
      "  -2.0105366e-02  1.4117502e-01]]\n",
      "2266\n",
      "['。', 'ばい', 'て', 'た', 'に', 'の', 'は', 'が', 'ね', 'あなた']\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.vectors.shape)  # 分散表現の形状\n",
    "print(model.wv.vectors)  # 分散表現\n",
    "print(len(model.wv.index2word))  # 語彙の数\n",
    "print(model.wv.index2word[:10])  # 最初の10単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('なるほど', 0.9403193593025208), ('お疲れ様', 0.9233492016792297), ('おやすみ', 0.9017361402511597), ('了解', 0.8463051319122314), ('おめでとう', 0.8451974987983704), ('育休', 0.8440346717834473), ('かえる', 0.8137755393981934), ('すい', 0.8121225833892822), ('恥ずかしい', 0.8101416230201721), ('暖かい', 0.8036526441574097)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"おはよう\"))  # 最も似ている単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents = []\n",
    "for i, sentence in enumerate(bot_words):\n",
    "    tagged_documents.append(TaggedDocument(sentence, [i]))  # TaggedDocument型のオブジェクトをリストに格納\n",
    "\n",
    "# size：分散表現の次元数\n",
    "# window：対象単語を中心とした前後の単語数\n",
    "# min_count：学習に使う単語の最低出現回数\n",
    "# epochs:epochs数\n",
    "# dm：学習モデル=DBOW（デフォルトはdm=1で、学習モデルはDM）\n",
    "model = Doc2Vec(documents=tagged_documents,\n",
    "                vector_size=100,\n",
    "                min_count=5,\n",
    "                window=5,\n",
    "                epochs=20,\n",
    "                dm=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['その', 'せい', 'で', '時間', 'なくなっ', 'て', 'ごめん', '。']\n",
      "[-0.11868197 -0.07130735 -0.01238266 -0.1575949  -0.09459867  0.01884246\n",
      "  0.01129084 -0.04596738 -0.07008135  0.00062519  0.1036119  -0.08251083\n",
      " -0.02306661  0.06548497  0.02011882  0.08888667  0.05701245  0.05906919\n",
      "  0.17237821 -0.06243692  0.20753194 -0.126915   -0.04976975 -0.00107312\n",
      "  0.00308469  0.10076164  0.08838043  0.0140352  -0.03272156 -0.11259116\n",
      " -0.05104002 -0.02882503 -0.05904601  0.03438864  0.01216092 -0.05625768\n",
      "  0.02501594 -0.05520113 -0.06955463  0.05748038  0.08389363 -0.08089931\n",
      " -0.01539682 -0.06214973 -0.00212878  0.08251083 -0.0352633   0.01160701\n",
      " -0.22429672  0.11489253 -0.00280632  0.07531861  0.07710977  0.14089033\n",
      "  0.08455271 -0.0330891   0.09535205 -0.03515467 -0.06135472 -0.08331118\n",
      "  0.06049691  0.02695638 -0.05867619  0.10592671 -0.02791326  0.09465496\n",
      "  0.09406038 -0.06744692 -0.03308775 -0.10167576  0.04034194 -0.16555281\n",
      "  0.00226399  0.06903473  0.07158112 -0.00146238  0.00131955  0.05770475\n",
      " -0.08065888 -0.07604624 -0.09927776 -0.04223318 -0.10756695  0.04160117\n",
      "  0.07127395  0.05198922 -0.04092001 -0.11240804 -0.06132494  0.0278335\n",
      "  0.01477245  0.08849514  0.00576412 -0.00068233  0.02159072  0.07830521\n",
      "  0.07978528 -0.03369281 -0.00512534  0.07504084]\n"
     ]
    }
   ],
   "source": [
    "print(bot_words[0])  # 最初の文章を表示\n",
    "print(model.docvecs[0])  # 最初の文章のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['スケジュール', '管理', 'でき', 'て', 'ない', '会社', 'が', '責任', '取る', 'べき', 'ばい', '。']\n",
      "['用事', '済ん', 'だら', '教え', 'て', '。']\n",
      "['徒歩', '35', '分', 'ばい', '。']\n",
      "['爽やか', 'スマイル', 'で', '好', '印象', '与え', 'て', 'いく', '。']\n",
      "['本格', '的', 'に', '引っ越し', 'の', '準備', 'し', 'てる', '。']\n",
      "['まあ', '許し', 'て', 'あげる', 'ばい', '。']\n",
      "['め', 'ま', '坊', 'ばい', '。']\n",
      "['豚カツ', 'おいしい', '。']\n",
      "['セブ', 'は', 'い', 'つ', '出発', '。']\n",
      "['明け', 'まし', 'て', 'おめでとう', '。']\n"
     ]
    }
   ],
   "source": [
    "for p in model.docvecs.most_similar(0):\n",
    "    print(bot_words[p[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
