{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from janome.tokenizer import Tokenizer\n",
    "import collections\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "with open(\"bot.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
    "    bot_original = f.read()\n",
    "\n",
    "bot = re.sub(\"[\\n]\", \"。\", bot_original) # | と全角半角スペース、「」と改行の削除\n",
    "bot = re.sub(\"[｜ 　]\", \"\", bot) # | と全角半角スペース、「」と改行の削除\n",
    "\n",
    "# 終わり記号の重複を除去\n",
    "bot = bot.replace('！。', '！')\n",
    "bot = bot.replace('？。', '？')\n",
    "bot = bot.replace('♪。', '♪')\n",
    "bot = bot.replace('？*', '？')\n",
    "bot = bot.replace('♪？', '♪')\n",
    "bot = bot.replace('。*', '。')\n",
    "\n",
    "seperator = \"。\"  # 。をセパレータに指定\n",
    "bot_list = re.split('[。！？♪]', bot)  # セパレーターを使って文章をリストに分割する\n",
    "bot_list.pop() # 最後の要素は空の文字列になるので、削除\n",
    "bot_list.pop(0)\n",
    "bot_list = [x+seperator for x in bot_list]  # 文章の最後に。を追加\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "bot_words = []\n",
    "for sentence in bot_list:\n",
    "    bot_words.append(t.tokenize(sentence, wakati=True))   # 文章ごとに単語に分割し、リストに格納\n",
    "    \n",
    "with open('bot_words.pickle', mode='wb') as f:  # pickleに保存\n",
    "    pickle.dump(bot_words, f)\n",
    "\n",
    "# 読み込み用\n",
    "# with open('bot_words.pickle', mode='rb') as f:\n",
    "#     bot_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size : 中間層のニューロン数\n",
    "# min_count : この値以下の出現回数の単語を無視\n",
    "# window : 対象単語を中心とした前後の単語数\n",
    "# iter : epochs数\n",
    "# sg : skip-gramを使うかどうか 0:CBOW 1:skip-gram\n",
    "model = word2vec.Word2Vec(bot_words,\n",
    "                          size=100,\n",
    "                          min_count=5,\n",
    "                          window=5,\n",
    "                          iter=20,\n",
    "                          sg = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vectors.shape)  # 分散表現の形状\n",
    "print(model.wv.vectors)  # 分散表現\n",
    "print(len(model.wv.index2word))  # 語彙の数\n",
    "print(model.wv.index2word[:10])  # 最初の10単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(\"おはよう\"))  # 最も似ている単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents = []\n",
    "for i, sentence in enumerate(bot_words):\n",
    "    tagged_documents.append(TaggedDocument(sentence, [i]))  # TaggedDocument型のオブジェクトをリストに格納\n",
    "\n",
    "# size：分散表現の次元数\n",
    "# window：対象単語を中心とした前後の単語数\n",
    "# min_count：学習に使う単語の最低出現回数\n",
    "# epochs:epochs数\n",
    "# dm：学習モデル=DBOW（デフォルトはdm=1で、学習モデルはDM）\n",
    "model = Doc2Vec(documents=tagged_documents,\n",
    "                vector_size=100,\n",
    "                min_count=5,\n",
    "                window=5,\n",
    "                epochs=20,\n",
    "                dm=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bot_words[0])  # 最初の文章を表示\n",
    "print(model.docvecs[0])  # 最初の文章のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.docvecs.most_similar(0):\n",
    "    print(bot_words[p[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
