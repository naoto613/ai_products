{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文字数: 280405\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "from pykakasi import kakasi\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GRU, Input, Masking\n",
    "from keras.callbacks import EarlyStopping \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "# 同じ階層に\"bot.txt\"を準備\n",
    "with open(\"bot.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
    "    bot_original = f.read()\n",
    "\n",
    "\n",
    "bot = re.sub(\"[\\n]\", \"。\", bot_original) # | と全角半角スペース、「」と改行の削除\n",
    "bot = re.sub(\"[｜ 　]\", \"\", bot) # | と全角半角スペース、「」と改行の削除\n",
    "\n",
    "bot = bot.replace('！。', '！')\n",
    "bot = bot.replace('？。', '？')\n",
    "bot = bot.replace('♪。', '♪')\n",
    "bot = bot.replace('？*', '？')\n",
    "bot = bot.replace('♪？', '♪')\n",
    "bot = bot.replace('。*', '。')\n",
    "\n",
    "print(\"文字数:\", len(bot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperator = \"。\"  # 。をセパレータに指定\n",
    "bot_list = re.split('[。！？♪]', bot)  # セパレーターを使って文章をリストに分割する\n",
    "bot_list.pop() # 最後の要素は空の文字列になるので、削除\n",
    "bot_list.pop(0)\n",
    "bot_list = [x+seperator for x in bot_list]  # 文章の最後に。を追加\n",
    "\n",
    "kakasi = kakasi()\n",
    "kakasi.setMode(\"J\", \"H\")  # J(漢字) からH(ひらがな)へ\n",
    "conv = kakasi.getConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x', 'け', 'す', '+', 'ギ', 'z', 'ニ', 'セ', 'ベ', 'げ', 'A', 'カ', 'ぞ', '2', '。', 'r', 'ど', 'ぁ', 'ゲ', 'ー', 'い', 'み', '3', 'm', '・', 'メ', 'J', 'G', 'ぢ', 'V', '\\t', 'ぎ', 'ダ', '！', 'わ', 'ウ', 'W', '.', 'ワ', 'ポ', 'も', 'H', 'ャ', 'Z', 'E', 'ぺ', 'グ', '4', 'ひ', 'ユ', '6', 'ブ', 'ケ', '5', 'B', 'め', 's', 'ナ', 'パ', 'し', 'コ', 'ヘ', 'u', 'な', 'ゴ', 'ふ', 'ご', 'や', '&', 'ぷ', '、', 'リ', 'キ', 'ツ', 'I', '♪', 'サ', 'i', 'T', 'せ', 'び', ':', 't', 'エ', '!', 'に', 'ゅ', 'ロ', 'ゃ', '0', 'f', 'さ', 'ず', 'ズ', 'あ', 'テ', 'ぬ', 'O', 'ん', 'ビ', 'w', '%', 'ゥ', 'ド', 'Q', 'ペ', 'ほ', 'ミ', 'ょ', 'g', 'だ', '-', 'X', 'つ', 'F', 'ボ', 'ク', 'ヨ', 'b', 'っ', 'ハ', 'ョ', 'デ', 'ァ', 'タ', 'ィ', 'ヒ', '\\ufeff', '〜', 'ン', 'く', 'h', 'ノ', 'y', 'ム', 'L', 'ェ', ',', '）', 'Ｄ', '‥', 'M', 'る', 'ガ', 'ヴ', 'ぅ', 'ぴ', 'ぽ', 'Y', 'ヵ', 'ね', 'n', 'そ', 'バ', 'ぜ', 'ぼ', 'l', 'モ', 'ち', 'S', 'ま', 'ホ', 'べ', 'マ', 'ネ', 'ぶ', 'づ', 'ュ', 'ヌ', 'ゾ', 'よ', 'ジ', '②', 'ォ', 'ソ', 'ト', 'は', '1', '9', 'P', 'Ｃ', 'れ', 'む', 'e', 'ゆ', 'ス', 'ピ', 'と', 'ぱ', 'こ', 'ざ', 'ぐ', 'a', 'ぃ', '，', 'て', 'q', 'シ', 'R', 'へ', 'ゼ', 'v', '（', 'の', 'フ', 'ら', 'チ', 'ヤ', 'を', 'プ', '7', 'ザ', 'う', 'ろ', 'り', 'え', 'お', 'ヶ', 'N', 'が', 'ル', 'イ', 'p', '…', 'か', 'オ', 'ラ', 'じ', 'で', 'ば', '～', 'ッ', 'た', '？', 'o', 'レ', 'き', 'ア', 'U'}\n"
     ]
    }
   ],
   "source": [
    "kana_text = conv.do(bot)  # 全体をひらがなに変換\n",
    "print(set(kana_text))  # set()で文字の重複をなくす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kana_bot.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(kana_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\t', '\\n', '!', '%', '&', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '9', ':', 'A', 'B', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‥', '…', '②', '♪', '、', '。', '〜', 'ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ', 'ま', 'み', 'む', 'め', 'も', 'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'ゎ', 'わ', 'ゐ', 'ゑ', 'を', 'ん', 'ァ', 'ア', 'ィ', 'イ', 'ゥ', 'ウ', 'ェ', 'エ', 'ォ', 'オ', 'カ', 'ガ', 'キ', 'ギ', 'ク', 'グ', 'ケ', 'ゲ', 'コ', 'ゴ', 'サ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ', 'ダ', 'チ', 'ヂ', 'ッ', 'ツ', 'ヅ', 'テ', 'デ', 'ト', 'ド', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'バ', 'パ', 'ヒ', 'ビ', 'ピ', 'フ', 'ブ', 'プ', 'ヘ', 'ベ', 'ペ', 'ホ', 'ボ', 'ポ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', 'ワ', 'ヰ', 'ヱ', 'ヲ', 'ン', 'ヴ', 'ヵ', 'ヶ', '・', 'ー', '\\ufeff', '！', '（', '）', '，', '？', 'Ｃ', 'Ｄ', '～']\n"
     ]
    }
   ],
   "source": [
    "hiragana = \"ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞ\\\n",
    "ただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽ\\\n",
    "まみむめもゃやゅゆょよらりるれろゎわゐゑをん\"\n",
    "\n",
    "katakana = \"ァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾ\\\n",
    "タダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポ\\\n",
    "マミムメモャヤュユョヨラリルレロヮワヰヱヲンヴ\"\n",
    "\n",
    "chars = hiragana + katakana\n",
    "\n",
    "with open(\"kana_bot.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # 前回保存したファイル\n",
    "    text = f.read()\n",
    "    \n",
    "for char in text:  # ひらがな、カタカナ以外でコーパスに使われている文字を追加\n",
    "    if char not in chars:\n",
    "        chars += char\n",
    "        \n",
    "chars += \"\\t\\n\"  # タブと改行を追加\n",
    "        \n",
    "chars_list = sorted(list(chars))  # 文字列をリストに変換してソートする\n",
    "print(chars_list)\n",
    "\n",
    "with open(\"kana_chars.pickle\", mode=\"wb\") as f:  # pickleで保存\n",
    "    pickle.dump(chars_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19047, 128, 252)\n"
     ]
    }
   ],
   "source": [
    "# インデックスと文字で辞書を作成\n",
    "char_indices = {}  # 文字がキーでインデックスが値\n",
    "for i, char in enumerate(chars_list):\n",
    "    char_indices[char] = i\n",
    "indices_char = {}  # インデックスがキーで文字が値\n",
    "for i, char in enumerate(chars_list):\n",
    "    indices_char[i] = char\n",
    "    \n",
    "seperator = \"。\"\n",
    "\n",
    "sentence_list = re.split('[。！？♪]', text)  # セパレーターを使って文章をリストに分割する\n",
    "sentence_list.pop() \n",
    "sentence_list = [x+seperator for x in sentence_list]\n",
    "\n",
    "max_sentence_length = 128  # 文章の最大長さ。これより長い文章はカットされる。\n",
    "sentence_list = [sentence for sentence in sentence_list if len(sentence) <= max_sentence_length]  # 長すぎる文章のカット\n",
    "\n",
    "n_char = len(chars_list)  # 文字の種類の数\n",
    "n_sample = len(sentence_list) - 1  # サンプル数\n",
    "\n",
    "x_sentences = []  # 入力の文章\n",
    "t_sentences = []  # 正解の文章\n",
    "for i in range(n_sample):\n",
    "    x_sentences.append(sentence_list[i])\n",
    "    t_sentences.append(\"\\t\" + sentence_list[i+1] + \"\\n\")  # 正解は先頭にタブ、末尾に改行を加える\n",
    "max_length_x = max_sentence_length  # 入力文章の最大長さ\n",
    "max_length_t = max_sentence_length + 2  # 正解文章の最大長さ\n",
    "\n",
    "x_encoder = np.zeros((n_sample, max_length_x, n_char), dtype=np.bool)  # encoderへの入力\n",
    "x_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderへの入力\n",
    "t_decoder = np.zeros((n_sample, max_length_t, n_char), dtype=np.bool)  # decoderの正解\n",
    "\n",
    "for i in range(n_sample):\n",
    "    x_sentence = x_sentences[i]\n",
    "    t_sentence = t_sentences[i]\n",
    "    for j, char in enumerate(x_sentence):\n",
    "        x_encoder[i, j, char_indices[char]] = 1  # encoderへの入力をone-hot表現で表す\n",
    "    for j, char in enumerate(t_sentence):\n",
    "        x_decoder[i, j, char_indices[char]] = 1  # decoderへの入力をone-hot表現で表す\n",
    "        if j > 0:  # 正解は入力より1つ前の時刻のものにする\n",
    "            t_decoder[i, j-1, char_indices[char]] = 1\n",
    "            \n",
    "print(x_encoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 150\n",
    "n_mid = 128  # 中間層のニューロン数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 252)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 252)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 252)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 252)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, 128), (None, 146304      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, None, 128),  146304      masking_2[0][0]                  \n",
      "                                                                 gru_1[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 252)    32508       gru_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 325,116\n",
      "Trainable params: 325,116\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input(shape=(None, n_char))\n",
    "encoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "encoder_masked = encoder_mask(encoder_input)\n",
    "encoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_state=True)  # dropoutを設定し、ニューロンをランダムに無効にする\n",
    "encoder_output, encoder_state_h = encoder_lstm(encoder_masked)\n",
    "\n",
    "decoder_input = Input(shape=(None, n_char))\n",
    "decoder_mask = Masking(mask_value=0)  # 全ての要素が0であるベクトルの入力は無視する\n",
    "decoder_masked = decoder_mask(decoder_input)\n",
    "decoder_lstm = GRU(n_mid, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=True)  # dropoutを設定\n",
    "decoder_output, _ = decoder_lstm(decoder_masked, initial_state=encoder_state_h)  # encoderの状態を初期状態にする\n",
    "decoder_dense = Dense(n_char, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17142 samples, validate on 1905 samples\n",
      "Epoch 1/150\n",
      "17142/17142 [==============================] - 161s 9ms/step - loss: 3.5518 - val_loss: 3.0041\n",
      "Epoch 2/150\n",
      "17142/17142 [==============================] - 148s 9ms/step - loss: 3.0277 - val_loss: 2.7362\n",
      "Epoch 3/150\n",
      "17142/17142 [==============================] - 148s 9ms/step - loss: 2.8685 - val_loss: 2.6393\n",
      "Epoch 4/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.7871 - val_loss: 2.5764\n",
      "Epoch 5/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.7305 - val_loss: 2.5217\n",
      "Epoch 6/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.6836 - val_loss: 2.4857\n",
      "Epoch 7/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.6471 - val_loss: 2.4502\n",
      "Epoch 8/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.6155 - val_loss: 2.4314\n",
      "Epoch 9/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.5854 - val_loss: 2.4048\n",
      "Epoch 10/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.5603 - val_loss: 2.3809\n",
      "Epoch 11/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.5348 - val_loss: 2.3602\n",
      "Epoch 12/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.5137 - val_loss: 2.3373\n",
      "Epoch 13/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.4945 - val_loss: 2.3238\n",
      "Epoch 14/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.4791 - val_loss: 2.3074\n",
      "Epoch 15/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.4609 - val_loss: 2.2988\n",
      "Epoch 16/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.4443 - val_loss: 2.2827\n",
      "Epoch 17/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.4349 - val_loss: 2.2753\n",
      "Epoch 18/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.4206 - val_loss: 2.2610\n",
      "Epoch 19/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.4076 - val_loss: 2.2546\n",
      "Epoch 20/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.3965 - val_loss: 2.2437\n",
      "Epoch 21/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3865 - val_loss: 2.2232\n",
      "Epoch 22/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3723 - val_loss: 2.2134\n",
      "Epoch 23/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3605 - val_loss: 2.2089\n",
      "Epoch 24/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3535 - val_loss: 2.1994\n",
      "Epoch 25/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3407 - val_loss: 2.1946\n",
      "Epoch 26/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.3359 - val_loss: 2.1897\n",
      "Epoch 27/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.3271 - val_loss: 2.1847\n",
      "Epoch 28/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3206 - val_loss: 2.1784\n",
      "Epoch 29/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3163 - val_loss: 2.1739\n",
      "Epoch 30/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3093 - val_loss: 2.1697\n",
      "Epoch 31/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.3025 - val_loss: 2.1645\n",
      "Epoch 32/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2971 - val_loss: 2.1612\n",
      "Epoch 33/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2907 - val_loss: 2.1605\n",
      "Epoch 34/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2874 - val_loss: 2.1554\n",
      "Epoch 35/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2833 - val_loss: 2.1515\n",
      "Epoch 36/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2786 - val_loss: 2.1492\n",
      "Epoch 37/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2749 - val_loss: 2.1457\n",
      "Epoch 38/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2679 - val_loss: 2.1442\n",
      "Epoch 39/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2640 - val_loss: 2.1419\n",
      "Epoch 40/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2590 - val_loss: 2.1397\n",
      "Epoch 41/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2559 - val_loss: 2.1367\n",
      "Epoch 42/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2544 - val_loss: 2.1333\n",
      "Epoch 43/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2504 - val_loss: 2.1317\n",
      "Epoch 44/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2457 - val_loss: 2.1305\n",
      "Epoch 45/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2440 - val_loss: 2.1289\n",
      "Epoch 46/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2427 - val_loss: 2.1240\n",
      "Epoch 47/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2361 - val_loss: 2.1250\n",
      "Epoch 48/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2335 - val_loss: 2.1258\n",
      "Epoch 49/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2296 - val_loss: 2.1201\n",
      "Epoch 50/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2276 - val_loss: 2.1188\n",
      "Epoch 51/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2255 - val_loss: 2.1214\n",
      "Epoch 52/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2237 - val_loss: 2.1163\n",
      "Epoch 53/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2214 - val_loss: 2.1157\n",
      "Epoch 54/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2198 - val_loss: 2.1156\n",
      "Epoch 55/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2143 - val_loss: 2.1128\n",
      "Epoch 56/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2142 - val_loss: 2.1116\n",
      "Epoch 57/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2126 - val_loss: 2.1141\n",
      "Epoch 58/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.2080 - val_loss: 2.1107\n",
      "Epoch 59/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2065 - val_loss: 2.1109\n",
      "Epoch 60/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2045 - val_loss: 2.1087\n",
      "Epoch 61/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2056 - val_loss: 2.1061\n",
      "Epoch 62/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.2010 - val_loss: 2.1081\n",
      "Epoch 63/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1987 - val_loss: 2.1058\n",
      "Epoch 64/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1981 - val_loss: 2.1037\n",
      "Epoch 65/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1951 - val_loss: 2.1034\n",
      "Epoch 66/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1928 - val_loss: 2.1080\n",
      "Epoch 67/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1912 - val_loss: 2.1014\n",
      "Epoch 68/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1914 - val_loss: 2.1010\n",
      "Epoch 69/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1865 - val_loss: 2.1010\n",
      "Epoch 70/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1865 - val_loss: 2.0997\n",
      "Epoch 71/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1842 - val_loss: 2.0969\n",
      "Epoch 72/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1832 - val_loss: 2.0974\n",
      "Epoch 73/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1814 - val_loss: 2.0975\n",
      "Epoch 74/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1806 - val_loss: 2.0971\n",
      "Epoch 75/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1791 - val_loss: 2.0959\n",
      "Epoch 76/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1750 - val_loss: 2.0953\n",
      "Epoch 77/150\n",
      "17142/17142 [==============================] - 145s 8ms/step - loss: 2.1784 - val_loss: 2.0955\n",
      "Epoch 78/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1737 - val_loss: 2.0942\n",
      "Epoch 79/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1746 - val_loss: 2.0956\n",
      "Epoch 80/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1688 - val_loss: 2.0950\n",
      "Epoch 81/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1694 - val_loss: 2.0932\n",
      "Epoch 82/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1671 - val_loss: 2.0907\n",
      "Epoch 83/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1686 - val_loss: 2.0928\n",
      "Epoch 84/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1665 - val_loss: 2.0910\n",
      "Epoch 85/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1647 - val_loss: 2.0924\n",
      "Epoch 86/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1645 - val_loss: 2.0911\n",
      "Epoch 87/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1638 - val_loss: 2.0896\n",
      "Epoch 88/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1614 - val_loss: 2.0886\n",
      "Epoch 89/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1599 - val_loss: 2.0909\n",
      "Epoch 90/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1572 - val_loss: 2.0882\n",
      "Epoch 91/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1563 - val_loss: 2.0897\n",
      "Epoch 92/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1585 - val_loss: 2.0868\n",
      "Epoch 93/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1610 - val_loss: 2.0888\n",
      "Epoch 94/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1537 - val_loss: 2.0886\n",
      "Epoch 95/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1549 - val_loss: 2.0895\n",
      "Epoch 96/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1513 - val_loss: 2.0887\n",
      "Epoch 97/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1507 - val_loss: 2.0873\n",
      "Epoch 98/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1514 - val_loss: 2.0873\n",
      "Epoch 99/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1483 - val_loss: 2.0858\n",
      "Epoch 100/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1461 - val_loss: 2.0844\n",
      "Epoch 101/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1454 - val_loss: 2.0868\n",
      "Epoch 102/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1483 - val_loss: 2.0852\n",
      "Epoch 103/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1448 - val_loss: 2.0866\n",
      "Epoch 104/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1457 - val_loss: 2.0868\n",
      "Epoch 105/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1438 - val_loss: 2.0876\n",
      "Epoch 106/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1437 - val_loss: 2.0852\n",
      "Epoch 107/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1408 - val_loss: 2.0837\n",
      "Epoch 108/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1408 - val_loss: 2.0844\n",
      "Epoch 109/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1399 - val_loss: 2.0837\n",
      "Epoch 110/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1394 - val_loss: 2.0849\n",
      "Epoch 111/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1370 - val_loss: 2.0850\n",
      "Epoch 112/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1370 - val_loss: 2.0844\n",
      "Epoch 113/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1376 - val_loss: 2.0831\n",
      "Epoch 114/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1333 - val_loss: 2.0823\n",
      "Epoch 115/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1311 - val_loss: 2.0843\n",
      "Epoch 116/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1318 - val_loss: 2.0813\n",
      "Epoch 117/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1303 - val_loss: 2.0837\n",
      "Epoch 118/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1334 - val_loss: 2.0813\n",
      "Epoch 119/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1294 - val_loss: 2.0817\n",
      "Epoch 120/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1325 - val_loss: 2.0834\n",
      "Epoch 121/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1292 - val_loss: 2.0808\n",
      "Epoch 122/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1297 - val_loss: 2.0812\n",
      "Epoch 123/150\n",
      "17142/17142 [==============================] - 142s 8ms/step - loss: 2.1276 - val_loss: 2.0817\n",
      "Epoch 124/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1258 - val_loss: 2.0792\n",
      "Epoch 125/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1261 - val_loss: 2.0798\n",
      "Epoch 126/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1265 - val_loss: 2.0810\n",
      "Epoch 127/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1247 - val_loss: 2.0803\n",
      "Epoch 128/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1279 - val_loss: 2.0793\n",
      "Epoch 129/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1222 - val_loss: 2.0811\n",
      "Epoch 130/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1242 - val_loss: 2.0813\n",
      "Epoch 131/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1243 - val_loss: 2.0827\n",
      "Epoch 132/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1225 - val_loss: 2.0787\n",
      "Epoch 133/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1248 - val_loss: 2.0831\n",
      "Epoch 134/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1216 - val_loss: 2.0806\n",
      "Epoch 135/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1214 - val_loss: 2.0814\n",
      "Epoch 136/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1192 - val_loss: 2.0823\n",
      "Epoch 137/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1189 - val_loss: 2.0788\n",
      "Epoch 138/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1166 - val_loss: 2.0767\n",
      "Epoch 139/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1172 - val_loss: 2.0814\n",
      "Epoch 140/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1170 - val_loss: 2.0805\n",
      "Epoch 141/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1167 - val_loss: 2.0770\n",
      "Epoch 142/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1175 - val_loss: 2.0792\n",
      "Epoch 143/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1137 - val_loss: 2.0762\n",
      "Epoch 144/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1151 - val_loss: 2.0785\n",
      "Epoch 145/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1125 - val_loss: 2.0797\n",
      "Epoch 146/150\n",
      "17142/17142 [==============================] - 144s 8ms/step - loss: 2.1142 - val_loss: 2.0796\n",
      "Epoch 147/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1155 - val_loss: 2.0777\n",
      "Epoch 148/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1123 - val_loss: 2.0776\n",
      "Epoch 149/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1113 - val_loss: 2.0788\n",
      "Epoch 150/150\n",
      "17142/17142 [==============================] - 143s 8ms/step - loss: 2.1140 - val_loss: 2.0763\n"
     ]
    }
   ],
   "source": [
    "# val_lossに改善が見られなくなってから、10エポックで学習は終了\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10) \n",
    "\n",
    "history = model.fit([x_encoder, x_decoder], t_decoder,\n",
    "                     batch_size=64,\n",
    "                     epochs=150,\n",
    "                     validation_split=0.1,  # 10%は検証用\n",
    "                     callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(np.arange(len(loss)), loss)\n",
    "plt.plot(np.arange(len(val_loss)), val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fujii\\Anaconda3\\envs\\tesorflow-gpu\\lib\\site-packages\\keras\\engine\\topology.py:2379: UserWarning: Layer gru_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# encoderのモデル\n",
    "encoder_model = Model(encoder_input, encoder_state_h)\n",
    "\n",
    "# decoderのモデル\n",
    "decoder_state_in_h = Input(shape=(n_mid,))\n",
    "decoder_state_in = [decoder_state_in_h]\n",
    "\n",
    "decoder_output, decoder_state_h = decoder_lstm(decoder_input,\n",
    "                                               initial_state=decoder_state_in_h)\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "decoder_model = Model([decoder_input] + decoder_state_in,\n",
    "                      [decoder_output, decoder_state_h])\n",
    "\n",
    "# モデルの保存\n",
    "encoder_model.save('encoder_model.h5')\n",
    "decoder_model.save('decoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(input_data, beta=5):\n",
    "    state_value = encoder_model.predict(input_data)\n",
    "    y_decoder = np.zeros((1, 1, n_char))  # decoderの出力を格納する配列\n",
    "    y_decoder[0][0][char_indices[\"\\t\"]] = 1  # decoderの最初の入力はタブ。one-hot表現にする。\n",
    "\n",
    "    respond_sentence = \"\"  # 返答の文字列\n",
    "    while True:\n",
    "        y, h = decoder_model.predict([y_decoder, state_value])\n",
    "        p_power = y[0][0] ** beta  # 確率分布の調整\n",
    "        next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power)) \n",
    "        next_char = indices_char[next_index]  # 次の文字\n",
    "\n",
    "        if (next_char == \"\\n\" or len(respond_sentence) >= max_length_x):\n",
    "            break  # 次の文字が改行のとき、もしくは最大文字数を超えたときは終了\n",
    "            \n",
    "        respond_sentence += next_char\n",
    "        y_decoder = np.zeros((1, 1, n_char))  # 次の時刻の入力\n",
    "        y_decoder[0][0][next_index] = 1\n",
    "\n",
    "        state_value = h  # 次の時刻の状態\n",
    "\n",
    "    return respond_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデックスと文字で辞書を作成\n",
    "char_indices = {}\n",
    "for i, char in enumerate(chars_list):\n",
    "    char_indices[char] = i\n",
    "indices_char = {}\n",
    "for i, char in enumerate(chars_list):\n",
    "    indices_char[i] = char\n",
    "    \n",
    "n_char = len(chars_list)\n",
    "max_length_x = 128\n",
    "\n",
    "# 文章をone-hot表現に変換する関数\n",
    "def sentence_to_vector(sentence):\n",
    "    vector = np.zeros((1, max_length_x, n_char), dtype=np.bool)\n",
    "    for j, char in enumerate(sentence):\n",
    "        vector[0][j][char_indices[char]] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fujii\\Anaconda3\\envs\\tesorflow-gpu\\lib\\site-packages\\keras\\models.py:282: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "encoder_model = load_model('encoder_model.h5')\n",
    "decoder_model = load_model('decoder_model.h5')\n",
    "\n",
    "def respond(input_data, beta=2):\n",
    "    state_value = encoder_model.predict(input_data)\n",
    "    y_decoder = np.zeros((1, 1, n_char))  # decoderの出力を格納する配列\n",
    "    y_decoder[0][0][char_indices['\\t']] = 1  # decoderの最初の入力はタブ。one-hot表現にする。\n",
    "\n",
    "    respond_sentence = \"\"  # 返答の文字列\n",
    "    while True:\n",
    "        y, h = decoder_model.predict([y_decoder, state_value])\n",
    "        p_power = y[0][0] ** beta  # 確率分布の調整\n",
    "        next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power)) \n",
    "        next_char = indices_char[next_index]  # 次の文字\n",
    "        \n",
    "        if (next_char == \"\\n\" or len(respond_sentence) >= max_length_x):\n",
    "            break  # 次の文字が改行のとき、もしくは最大文字数を超えたときは終了\n",
    "            \n",
    "        respond_sentence += next_char\n",
    "        y_decoder = np.zeros((1, 1, n_char))  # 次の時刻の入力\n",
    "        y_decoder[0][0][next_index] = 1\n",
    "\n",
    "        state_value = h  # 次の時刻の状態\n",
    "\n",
    "    return respond_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A君: こんにちは。\n",
      "\n",
      "B君: こんにちはあなたがないばい。\n",
      "\n",
      "A君: あなたがきょうどうばい。\n",
      "\n",
      "B君: またあなたがすごいばい。\n",
      "\n",
      "A君: あなたのきょうのかんかえるばい。\n",
      "\n",
      "B君: そうばいね。\n",
      "\n",
      "A君: いいよね。\n",
      "\n",
      "B君: あなたはそれならかえったらだいじょうぶだとおもう。\n",
      "\n",
      "A君: いまからでんしゃできた。\n",
      "\n",
      "B君: ドレスきていこう。\n",
      "\n",
      "A君: つかれてるね。\n",
      "\n",
      "B君: おつかれさま。\n",
      "\n",
      "A君: あしたもおわった。\n",
      "\n",
      "B君: うん。\n",
      "\n",
      "A君: かいしゃはいいね。\n",
      "\n",
      "B君: おはよう。\n",
      "\n",
      "A君: あなたもおんせんしてるからあなたがよくねる。\n",
      "\n",
      "B君: そうだね。\n",
      "\n",
      "A君: いまからがんばる。\n",
      "\n",
      "B君: あなたがいさんときにいく。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_a = \"こんにちは。\"\n",
    "response_b = \"\"\n",
    "for i in range(10):\n",
    "    print(\"A君:\", response_a)\n",
    "    print()  \n",
    "    vector_a = sentence_to_vector(response_a)\n",
    "    \n",
    "    response_b = respond(vector_a)\n",
    "    print(\"B君:\", response_b)\n",
    "    print()\n",
    "    vector_b = sentence_to_vector(response_b)\n",
    "    \n",
    "    response_a = respond(vector_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
